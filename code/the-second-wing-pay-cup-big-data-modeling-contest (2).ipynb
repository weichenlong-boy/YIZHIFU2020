{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install toad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n#         elif t == np.object:\n#             if cols[i] == 'date':\n#                 df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n#             else:\n#                 df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport gc\nimport numpy as np\nimport catboost as cbt\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom datetime import datetime, timedelta\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew, norm\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import boxcox_normmax\nimport category_encoders as ce\nfrom collections import Counter\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import  roc_auc_score,accuracy_score\nimport warnings\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n#import toad\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 读取数据\ntrain_op = pd.read_csv('/kaggle/input/yizhifu-7-20/train_op.csv')\ndf_train_label = pd.read_csv('/kaggle/input/yizhifu-7-20/train_label.csv')\ndf_train_base = pd.read_csv('/kaggle/input/yizhifu-7-20/train_base.csv')\ndf_train_trans = pd.read_csv('/kaggle/input/yizhifu-7-20/train_trans.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"testA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_op = pd.read_csv('../input/yizhifu-7-20/test_a_op.csv')\ndf_test_base = pd.read_csv('../input/yizhifu-7-20/test_a_base.csv')\ndf_test_trans = pd.read_csv('../input/yizhifu-7-20/test_a_trans.csv')\nsubmit = pd.read_csv('../input/yizhifu-7-20/submit_example.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"testB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_op = pd.read_csv('../input/fusaishuju/testb_op.csv')\ndf_test_base = pd.read_csv('../input/fusaishuju/testb_base.csv')\ndf_test_trans = pd.read_csv('../input/fusaishuju/testb_trans.csv')\nsubmit = pd.read_csv('../input/fusaishuju/submit_example.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans = df_train_trans.append(df_test_trans)\ndf_trans = df_trans.reset_index(drop=True)\ndf_op = train_op.append(test_op)\ndf_op = df_op.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_time(tm):\n    days, _, time = tm.split(' ')\n    time = time.split('.')[0]\n    time = '2020-1-1 ' + time\n    time = datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n    time = (time + timedelta(days=int(days)))\n    return time\n\ndf_trans['date'] = df_trans['tm_diff'].apply(parse_time)\ndf_op['date'] = df_op['tm_diff'].apply(parse_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_deal(train_te):\n    # 日\n    train_te['day']=train_te['date'].apply(lambda x: x.day)\n    # 小时\n    train_te['hour']=train_te['date'].apply(lambda x: x.hour)\n    # 分钟\n    train_te['minute']=train_te['date'].apply(lambda x: x.minute)\n    # 秒数\n    train_te['Seconds']=train_te['date'].apply(lambda x: x.second)\n    # 一天中的第几分钟\n    train_te['Minutes_of_the_day']=train_te['date'].apply(lambda x: x.minute + x.hour*60)\n    # 星期几\n    train_te['Day_of_the_week']=train_te['date'].apply(lambda x: x.dayofweek )\n    return train_te\ndf_trans = date_deal(df_trans)\ndf_op = date_deal(df_op)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_op.sort_values(['user', 'date'], inplace=True)\ndf_op = df_op.reset_index(drop=True)\ndf_trans.sort_values(['user', 'date'], inplace=True)\ndf_trans = df_trans.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train_base.merge(df_train_label, how='left')\ndf_test = df_test_base\ndf_feature = df_train.append(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_ft(df_feature1 = df_feature):\n    df_temp = df_trans.groupby(['user'])['amount'].agg(amount_mean='mean',amount_std='std',amount_sum='sum',amount_max='max',amount_min='min',amount_med='median',amount_cnt='count').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['day'].agg(day_mean='mean',day_std='std',day_sum='sum',day_max='max',day_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['hour'].agg(hour_mean='mean',hour_std='std',hour_sum='sum',hour_max='max',hour_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['minute'].agg(minute_mean='mean',minute_std='std',minute_sum='sum',minute_max='max',minute_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['Seconds'].agg(Seconds_mean='mean',Seconds_std='std',Seconds_sum='sum',Seconds_max='max',Seconds_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['Minutes_of_the_day'].agg(Minutes_of_the_day_mean='mean',Minutes_of_the_day_std='std',Minutes_of_the_day_sum='sum',Minutes_of_the_day_max='max',Minutes_of_the_day_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_trans.groupby(['user'])['Day_of_the_week'].agg(Day_of_the_week_mean='mean',Day_of_the_week_std='std',Day_of_the_week_sum='sum',Day_of_the_week_max='max',Day_of_the_week_min='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n#     df_temp = df_trans.groupby(['user'])['type_proportion'].agg(Day_of_the_week_mean='mean',Day_of_the_week_std='std',Day_of_the_week_sum='sum',Day_of_the_week_max='max',Day_of_the_week_min='min').reset_index()\n#     df_feature1 = df_feature1.merge(df_temp, how='left')\n    \n    return df_feature1\ndf_feature = count_ft()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count1_ft(df_feature1 = df_feature):\n    df_temp = df_op.groupby(['user'])['day'].agg(day_mean1='mean',day_std1='std',day_sum1='sum',day_max1='max',day_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_op.groupby(['user'])['hour'].agg(hour_mean1='mean',hour_std1='std',hour_sum1='sum',hour_max1='max',hour_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_op.groupby(['user'])['minute'].agg(minute_mean1='mean',minute_std1='std',minute_sum1='sum',minute_max1='max',minute_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_op.groupby(['user'])['Seconds'].agg(Seconds_mean1='mean',Seconds_std1='std',Seconds_sum1='sum',Seconds_max1='max',Seconds_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_op.groupby(['user'])['Minutes_of_the_day'].agg(Minutes_of_the_day_mean1='mean',Minutes_of_the_day_std1='std',Minutes_of_the_day_sum1='sum',Minutes_of_the_day_max1='max',Minutes_of_the_day_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    df_temp = df_op.groupby(['user'])['Day_of_the_week'].agg(Day_of_the_week_mean1='mean',Day_of_the_week_std1='std',Day_of_the_week_sum1='sum',Day_of_the_week_max1='max',Day_of_the_week_min1='min').reset_index()\n    df_feature1 = df_feature1.merge(df_temp, how='left')\n    \n    return df_feature1\ndf_feature = count1_ft()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans['days_diff'] = df_trans['tm_diff'].apply(lambda x: int(x.split(' ')[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def w2v_feat(df, group_id, feat, length):\n    print('start word2vec ...')\n    df[feat] = df[feat].astype('str')\n    data_frame = df.groupby(group_id)[feat].agg(list).reset_index()\n    model = Word2Vec(data_frame[feat].values, size=length, window=5, min_count=1, sg=1,workers=1, hs=1, iter=10, seed=1)\n    data_frame[feat] = data_frame[feat].apply(lambda x: pd.DataFrame([model[c] for c in x]))\n    for m in range(length): \n        data_frame['w2v_{}_mean'.format(m)] = data_frame[feat].apply(lambda x: x[m].mean())\n    del data_frame[feat]\n    return data_frame\ndf_feature = df_feature.merge(w2v_feat(df_trans, 'user', 'amount', 32), on=['user'], how='left')\ndf_feature = df_feature.merge(w2v_feat(df_op, 'user', 'op_device', 32), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = df_feature.merge(w2v_feat(df_op, 'user', 'ip', 32), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def lda_feature(prefix,df, groupby, target,n_topic):\n#     df_bag = pd.DataFrame(df[[groupby, target]])\n#     df_bag[target] = df_bag[target].astype(str)\n#     df_bag[target].fillna('NAN', inplace=True)    \n#     df_bag = df_bag.groupby(groupby, as_index=False)[target].agg({'list':(lambda x: list(x))})\n#     df_bag['sentence'] = df_bag['list'].apply(lambda x: list(map(str,x)))\n#     docs = df_bag['sentence'].tolist() \n#     dictionary = corpora.Dictionary(docs)\n#     corpus = [dictionary.doc2bow(text) for text in docs]\n#     lda = LdaMulticore(corpus, id2word=dictionary, num_topics=n_topic)\n#     docres = [dict(lda[doc_bow]) for doc_bow in corpus]\n#     df_lda = pd.DataFrame(docres,dtype=np.float16).fillna(0.001)\n#     df_lda.columns = ['lda_%s_%s_%d'%(prefix,target,x) for x in range(n_topic)]\n#     df_lda[groupby] = df_bag[groupby]\n#     print ('df_lda:' + str(df_lda.shape))\n#     return df_lda\n# df_feature = df_feature.merge(lda_feature('lda',df_trans, 'user', 'amount', 10), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_feature['amount_per_days'] = df_feature['amount_sum'] / df_feature['user_trans_days_diff_nuniq']\n#df_feature['amount_per_cnt'] = df_feature['amount_sum'] / df_feature['amount_cnt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group_df = df_trans[df_trans['type1']=='45a1168437c708ff'].groupby(['user'])['days_diff'].agg(user_type1_45a1168437c708ff_min_day= 'min').reset_index()\n# df_feature = df_feature.merge(group_df, on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def gen_user_null_features(df, value, prefix):\n#     df['is_null'] = 0\n#     df.loc[df[value].isnull(), 'is_null'] = 1\n#     group_df = df.groupby(['user'])['is_null'].agg(user_trans_ip_null_cnt= 'sum',\n#                                                    user_trans_ip_null_ratio= 'mean').reset_index()\n#     return group_df\n# df_feature = df_feature.merge(gen_user_null_features(df=df_trans, value='ip', prefix='trans'), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_user_tfidf_features(df, value):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = TfidfVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\ndef gen_user_countvec_features(df, value):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = CountVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = df_feature.merge(gen_user_tfidf_features(df=df_op, value='op_mode'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_tfidf_features(df=df_op, value='op_type'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_countvec_features(df=df_op, value='op_mode'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_countvec_features(df=df_op, value='op_type'), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = df_feature.merge(gen_user_tfidf_features(df=df_op, value='ip'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_countvec_features(df=df_op, value='ip'), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_user_group_amount_features(df, value):\n    group_df = df.pivot_table(index='user',\n                              columns=value,\n                              values='amount',\n                              dropna=False,\n                              aggfunc=['count', 'sum']).fillna(0)\n    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n    group_df.reset_index(inplace=True)\n    return group_df \ndf_trans['amount'] = df_trans['amount'].astype('int')\ndf_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='platform'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='type1'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='type2'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='tunnel_in'), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='tunnel_out'), on=['user'], how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='type1'), on=['user'], how='left')\n# df_feature = df_feature.merge(gen_user_group_amount_features(df=df_trans, value='type2'), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_user_window27_amount_features(df, window):\n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        user_amount_mean_27 = 'mean',\n        user_amount_std_27 = 'std',\n        user_amount_max_27 = 'max',\n        user_amount_min_27 = 'min',\n        user_amount_sum_27 = 'sum',\n        user_amount_med_27 = 'median',\n        user_amount_cnt_27 = 'count',\n        ).reset_index()\n    return group_df\ndef gen_user_window23_amount_features(df, window):\n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        user_amount_mean_23 = 'mean',\n        user_amount_std_23 = 'std',\n        user_amount_max_23 = 'max',\n        user_amount_min_23 = 'min',\n        user_amount_sum_23 = 'sum',\n        user_amount_med_23 = 'median',\n        user_amount_cnt_23 = 'count',\n        ).reset_index()\n    return group_df\ndef gen_user_window15_amount_features(df, window):\n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        user_amount_mean_15 = 'mean',\n        user_amount_std_15 = 'std',\n        user_amount_max_15 = 'max',\n        user_amount_min_15 = 'min',\n        user_amount_sum_15 = 'sum',\n        user_amount_med_15 = 'median',\n        user_amount_cnt_15 = 'count',\n        ).reset_index()\n    return group_df\n\ndf_trans['amount'] = df_trans['amount'].astype('int')\ndf_trans['days_diff'] = df_trans['days_diff'].astype('int')\ndf_feature = df_feature.merge(gen_user_window27_amount_features(df=df_trans, window=27), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_window23_amount_features(df=df_trans, window=23), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_window15_amount_features(df=df_trans, window=15), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_window15_amount_features(df=df_trans, window=9), on=['user'], how='left')\ndf_feature = df_feature.merge(gen_user_window15_amount_features(df=df_trans, window=2), on=['user'], how='left')\n#df_feature = df_feature.merge(gen_user_window185_amount_features(df=df_trans, window1=5,window2=28), on=['user'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in [\n        'balance', 'balance_avg', 'balance1', 'balance1_avg', 'balance2',\n        'balance2_avg', 'product1_amount', 'product2_amount',\n        'product3_amount', 'product4_amount', 'product5_amount', 'product6_amount'\n]:\n    df_feature[f] = df_feature[f].apply(lambda x: int(x.split(' ')[1]) if type(x) != float else np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature['product7_fail_ratio'] = df_feature['product7_fail_cnt'] / df_feature['product7_cnt']\ndf_feature['card_cnt'] = df_feature['card_a_cnt'] + df_feature['card_b_cnt'] + df_feature['card_c_cnt'] + df_feature['card_d_cnt']\ndf_feature['acc_card_ratio'] = df_feature['acc_count'] / df_feature['card_cnt']\ndf_feature['login_cnt'] = df_feature['login_cnt_period1'] + df_feature['login_cnt_period2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature['city_level'] = df_feature['city'].map(str) + '_' + df_feature['level'].map(str)\ndf_feature['city_balance_avg'] = df_feature['city'].map(str) + '_' + df_feature['balance_avg'].map(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stat(df, df_merge, group_by, agg):\n    group = df.groupby(group_by).agg(agg)\n    columns = []\n    for on, methods in agg.items():\n        for method in methods:\n            columns.append('{}_{}_{}'.format('_'.join(group_by), on, method))\n    group.columns = columns\n    group.reset_index(inplace=True)\n    df_merge = df_merge.merge(group, on=group_by, how='left')\n    del (group)\n    gc.collect()\n    return df_merge\n\ndef statis_feat(df_know, df_unknow):   \n    df_unknow = stat(df_know, df_unknow, ['province'], {'label': ['mean']})\n    df_unknow = stat(df_know, df_unknow, ['city'], {'label': ['mean']})\n    df_unknow = stat(df_know, df_unknow, ['city_level'], {'label': ['mean']})\n    df_unknow = stat(df_know, df_unknow, ['city_balance_avg'], {'label': ['mean']})\n#     df_unknow = stat(df_know, df_unknow, ['cross_province_level'], {'label': ['mean']})\n#     df_unknow = stat(df_know, df_unknow, ['cross_province_verified'], {'label': ['mean']})\n#     df_unknow = stat(df_know, df_unknow, ['cross_province_regist_type'], {'label': ['mean']})\n#     df_unknow = stat(df_know, df_unknow, ['cross_province_service3'], {'label': ['mean']})\n    return df_unknow\n\ndf_train = df_feature[~df_feature['label'].isnull()]\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_feature[df_feature['label'].isnull()]\n\ndf_stas_feat = None\nkf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\nfor train_index, val_index in kf.split(df_train, df_train['label']):\n    df_fold_train = df_train.iloc[train_index]\n    df_fold_val = df_train.iloc[val_index]\n    df_fold_val = statis_feat(df_fold_train, df_fold_val)\n    df_stas_feat = pd.concat([df_stas_feat, df_fold_val], axis=0)\n    del (df_fold_train)\n    del (df_fold_val)\n    gc.collect()\n\ndf_test = statis_feat(df_train, df_test)\ndf_feature = pd.concat([df_stas_feat, df_test], axis=0)\ndf_feature = df_feature.reset_index(drop=True)\n\ndel (df_stas_feat)\ndel (df_train)\ndel (df_test)\ndel df_feature['service3_level']\ndel df_feature['city_level']\ndel df_feature['city_balance_avg']\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for c in tqdm(['sex','provider','level','verified','regist_type','service3']):\n#     for c1 in ['sex','provider','level','verified','regist_type','service3']:\n#         if c1==c:\n#             continue\n#         if 'cross_{}_{}'.format(c,c1) not in df_feature.columns and 'cross_{}_{}'.format(c1,c) not in df_feature.columns:\n#             df_feature['cross_{}_{}'.format(c,c1)]=df_feature[c].astype(str)+df_feature[c1].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm([i for i in df_feature.select_dtypes('object').columns if i != 'user']):\n    df_feature[f'{i}_count'] = df_feature.groupby([i])['user'].transform('count')\n    \n# cross_cols = [i for i in df_feature.select_dtypes('object').columns if i != 'user']\n# for f in tqdm(cross_cols):\n#     for col in cross_cols:\n#         if col == f:\n#             continue\n#         if  'cross_{}_{}_nunique'.format(f, col) not in df_feature.columns.values:\n#             df_feature['cross_{}_{}_nunique'.format(f, col)] = df_feature.groupby(f)[col].transform('nunique')\n#         if 'cross_{}_{}_count'.format(f, col) not in df_feature.columns.values and 'cross_{}_{}_count'.format(col, f) not in df_feature.columns.values:\n#             df_feature['cross_{}_{}_count_ratio'.format(f, col)] = df_feature.groupby([f,col])['user'].transform('count')/df_feature[f + '_count']\n#         if 'cross_{}_{}_count_ratio'.format(col, f) not in df_feature.columns.values:\n#             df_feature['cross_{}_{}_count_ratio'.format(col, f)] = df_feature['cross_{}_{}_count'.format(f, col)] / df_feature[f + '_count'] # 比例偏好\n#         if 'cross_{}_{}_count_ratio'.format(f, col) not in df_feature.columns.values:\n#             df_feature['cross_{}_{}_count_ratio'.format(f, col)] = df_feature['cross_{}_{}_count'.format(f, col)] / df_feature[col + '_count'] # 比例偏好\n#         df_feature['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df_feature['cross_{}_{}_nunique'.format(f, col)] / df_feature[f + '_count']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in tqdm(df_feature.select_dtypes('object').columns):\n    if f not in ['user']:\n        lbl = LabelEncoder()\n        df_feature[f] = lbl.fit_transform(df_feature[f].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature['rate_agreement'] = (df_feature['agreement1']+df_feature['agreement2']+df_feature['agreement3']+df_feature['agreement4'])/df_feature['agreement_total']\ndf_feature['cart'] = df_feature['sex']+df_feature['age']+df_feature['balance']+df_feature['regist_type']\ndf_feature['cart1'] = df_feature['sex']+df_feature['age']\ndf_feature['cart2'] = df_feature['sex']+df_feature['balance']+df_feature['regist_type']\ndf_feature['cart2'] = df_feature['balance']+df_feature['regist_type']\ndf_feature['card_a_rate'] = df_feature['card_a_cnt'] / df_feature['card_cnt'] \ndf_feature['card_b_rate'] = df_feature['card_b_cnt'] / df_feature['card_cnt'] \ndf_feature['card_c_rate'] = df_feature['card_c_cnt'] / df_feature['card_cnt'] \ndf_feature['card_d_rate'] = df_feature['card_d_cnt'] / df_feature['card_cnt'] \ndf_feature['op_cnt'] = df_feature['op1_cnt'] + df_feature['op2_cnt'] \ndf_feature['op1_cnt_rate'] = df_feature['op1_cnt'] / df_feature['op_cnt'] \ndf_feature['op2_cnt_rate'] = df_feature['op2_cnt'] / df_feature['op_cnt'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_feature[~df_feature['label'].isnull()].copy()\ndf_test = df_feature[df_feature['label'].isnull()].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_feature[:df_train_base.shape[0]]\ndf_test = df_feature[df_train_base.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(df_train['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.to_csv('df_feature_test1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_feature['province_nunique_regist_type'] = df_feature.groupby('province')['regist_type'].transform('nunique')\n# df_feature['province_nunique_provider'] = df_feature.groupby('province')['provider'].transform('nunique')\n# df_feature['login1_cnt_rate'] = df_feature['login_cnt_period1'] / df_feature['login_cnt']\n# df_feature['login2_cnt_rate'] = df_feature['login_cnt_period2'] / df_feature['login_cnt']\n# df_feature['balance1_rate'] = df_feature['balance1']/df_feature['balance1_avg']\n# df_feature['balance2_rate'] = df_feature['balance2']/df_feature['balance2_avg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 贪婪算法lr筛选特征","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# \"\"\"\n# Greedy Feature Selection using Logistic Regression as base model\n# to optimize Area Under the ROC Curve\n\n# \"\"\"\n# import numpy as np\n\n# import sklearn.linear_model as lm\n# from sklearn import metrics, preprocessing\n\n\n# class greedyFeatureSelection(object):\n\n#     def __init__(self, data, labels, scale=1, verbose=0):\n#         if scale == 1:\n#             self._data = preprocessing.scale(np.array(data))\n#         else:\n#             self._data = np.array(data)\n#         self._labels = labels\n#         self._verbose = verbose\n\n#     def evaluateScore(self, X, y):\n#         x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020, stratify=y)\n#         model = lm.LogisticRegression()\n#         model.fit(x_train, y_train)\n#         predictions = model.predict_proba(x_test)[:, 1]\n#         auc = metrics.roc_auc_score(y_test, predictions)\n#         return auc\n\n#     def selectionLoop(self, X, y):\n#         score_history = []\n#         good_features = set([])\n#         num_features = X.shape[1]\n#         while len(score_history) < 2 or score_history[-1][0] > score_history[-2][0]:\n#             scores = []\n#             for feature in range(num_features):\n#                 if feature not in good_features:\n#                     selected_features = list(good_features) + [feature]\n\n#                     Xts = np.column_stack(X[:, j] for j in selected_features)\n\n#                     score = self.evaluateScore(Xts, y)\n#                     scores.append((score, feature))\n\n#                     if self._verbose:\n#                         print (\"Current AUC : \", np.mean(score))\n\n#             good_features.add(sorted(scores)[-1][1])\n#             score_history.append(sorted(scores)[-1])\n#             if self._verbose:\n#                 print (\"Current Features : \", sorted(list(good_features)))\n\n#         # Remove last added feature\n#         good_features.remove(score_history[-1][1])\n#         good_features = sorted(list(good_features))\n#         if self._verbose:\n#             print (\"Selected Features : \", good_features)\n\n#         return good_features\n\n#     def transform(self, X):\n#         X = self._data\n#         y = self._labels\n#         good_features = self.selectionLoop(X, y)\n#         return X[:, good_features]\n\n# col = [i for i in df_train.columns if i not in ['user','label']]\n# X_train = df_train[col]\n# #X_train['oof'] = oof\n# y = df_train['label']\n# X_test = df_test[col]\n# X_train.fillna(X_train.mean(),inplace=True)\n# X_test.fillna(X_test.mean(),inplace=True)\n# x = greedyFeatureSelection(data=X_train, labels=y,verbose=1)\n# X = x.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = pd.read_csv('../input/df-feature-yizhifu/df_feature_test2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#初赛伪标签\ndf_feature = pd.read_csv('../input/df-feature-yizhifu/df_feature_test1.csv')\ndf_test_1 = df_feature[df_feature['label'].isnull()].copy()\nsubmits = pd.read_csv('../input/1231412/file00525306.csv')\ndf_test1 = df_test_1.merge(submits, on=['user'], how='left')\ndf1 = df_test1[df_test1['prob']>0.840]\ndf1['label'] = 1\ndel df1['prob']\ndf_train1 = pd.concat([df1,df_train],axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#复赛伪标签\nsubmits = pd.read_csv('../input/1231412/submit1 (28).csv')\ndf_test1 = df_test.merge(submits, on=['user'], how='left')\ndf1 = df_test1[df_test1['prob']>0.840]\ndf1['label'] = 1\ndel df1['prob']\ndf_train2 = pd.concat([df1,df_train],axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ####catboost模型融合了五个种子会比较慢\n# cat = [i for i in df_train.columns if df_train[i].dtype=='object' and i not in ['set','user','label']]\n# col = [i for i in df_train.columns if i not in ['user','set','label']+cat]\n# for f in tqdm(col):\n#     df_train[f] = df_train[f].fillna(-1).astype('int32')\n#     df_test[f] = df_test[f].fillna(-1).astype('int32')\n# for f in tqdm(cat):\n#     df_train[f] = df_train[f].fillna(-1)\n#     df_test[f] = df_test[f].fillna(-1)\n# X_train = df_train[col+cat]\n# y = df_train['label']\n# X_test = df_test[col+cat]\n\n\n# print(X_train.shape,X_test.shape)\n# oof = np.zeros(X_train.shape[0])\n# prediction = np.zeros(X_test.shape[0])\n# seeds = [1024]#19970412, 2019 * 2 + 1024, 4096, 2048, \n# num_model_seed = 1\n# for model_seed in range(num_model_seed):\n#     oof_cat = np.zeros(X_train.shape[0])\n#     prediction_cat=np.zeros(X_test.shape[0])\n#     skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n#     for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n#         print(index)\n#         train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n#         cbt_model = cbt.CatBoostClassifier(iterations=10000,learning_rate=0.5,verbose=100,max_depth=7,\n#                                        early_stopping_rounds=100,task_type='GPU',eval_metric='AUC',\n                                      \n#                                        )#\n#         cbt_model.fit(train_x, train_y  ,eval_set=(test_x,test_y),cat_features=cat+col)\n#         oof_cat[test_index] += cbt_model.predict_proba(test_x)[:,1]\n#         prediction_cat += cbt_model.predict_proba(X_test)[:,1]/5\n#     print('AUC',roc_auc_score(y, oof_cat))\n#     oof += oof_cat / num_model_seed\n#     prediction += prediction_cat / num_model_seed\n# print('AUC',roc_auc_score(y, oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # 3层神经网络模型融合了五个种子\n# col = [i for i in df_train.columns if i not in ['user','label']]\n# X_train = df_train1[col]\n# #X_train['oof'] = oof\n# y = df_train1['label']\n# X_test = df_test[col]\n\n# def formatTrans(df,columns):\n#     for i in col:\n#         if df[i].dtypes == 'float64':\n#             df[i] = df[i].astype(np.float32)\n#         if df[i].dtypes == 'int64':\n#             df[i] = df[i].astype(np.int32)\n# formatTrans(X_train,col)\n# formatTrans(X_test,col)\n\n# scalar=StandardScaler() \n# for i in col:\n#     X_train[i]=scalar.fit_transform(X_train[i].values.reshape(-1, 1))\n#     X_test[i]=scalar.transform(X_test[i].values.reshape(-1, 1))\n# # 数据归一化\n# # scalar=MinMaxScaler()\n# # X_train = scalar.fit_transform(X_train)\n# # X_test = scalar.transform(X_test)\n# X_train.fillna(-1,inplace=True)\n# X_test.fillna(-1,inplace=True)\n# # 3层神经网络\n# from sklearn.neural_network import MLPClassifier\n# seeds = [19970412, 2020 * 2 + 1024, 4096, 2048,1024]# \n# num_model_seed = 5\n# for i,model_seed in enumerate(range(num_model_seed)):\n#     oof_cat = np.zeros(X_train.shape[0])\n#     prediction_cat=np.zeros(X_test.shape[0])\n#     skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n#     print(f'第{i}个种子：{model_seed}')\n#     for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n#         train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n#         dnn = MLPClassifier(hidden_layer_sizes =(2**10,2**9,2**5),verbose=True,max_iter=1000,random_state=2020)                    # criterion='mae'\n#         dnn.fit(train_x,train_y)\n#         oof_cat[test_index] += dnn.predict_proba(test_x)[:,1]\n#         prediction_cat += dnn.predict_proba(X_test)[:, 1]/5\n#     print('AUC',roc_auc_score(y, oof_cat)) \n#     prediction += prediction_cat / num_model_seed\n# print('AUC',roc_auc_score(y, oof))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # svm模型融合了五个种子\n\n# from sklearn import svm\n# seeds = [19970412, 2020 * 2 + 1024, 4096, 2048,1024]# \n# num_model_seed = 5\n# for i,model_seed in enumerate(range(num_model_seed)):\n#     oof_cat = np.zeros(X_train.shape[0])\n#     prediction_cat=np.zeros(X_test.shape[0])\n#     skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n#     print(f'第{i}个种子：{model_seed}')\n#     for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n#         print(index)\n#         train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n#         dnn = svm.SVC(probability = True,random_state=2020)                    # criterion='mae'\n#         dnn.fit(train_x,train_y)\n#         oof_cat[test_index] += dnn.predict_proba(test_x)[:,1]\n#         prediction_cat += dnn.predict_proba(X_test)[:, 1]/5\n#     print('AUC',roc_auc_score(y, oof_cat)) \n#     prediction += prediction_cat / num_model_seed\n# print('AUC',roc_auc_score(y, oof))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# # 3层神经网络\n# import numpy as np\n# import pandas as pd\n# from keras.models import Sequential, Model\n# from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout, concatenate\n# from keras.callbacks import ModelCheckpoint\n# import tensorflow as tf\n# import random\n\n\n# col = [i for i in df_train.columns if i not in ['user','label']]\n# X_train = df_train[col]\n# #X_train['oof'] = oof\n# y = df_train['label']\n# X_test = df_test[col]\n\n# def formatTrans(df,columns):\n#     for i in col:\n#         if df[i].dtypes == 'float64':\n#             df[i] = df[i].astype(np.float32)\n#         if df[i].dtypes == 'int64':\n#             df[i] = df[i].astype(np.int32)\n# formatTrans(X_train,col)\n# formatTrans(X_test,col)\n\n# scalar=StandardScaler() \n# for i in col:\n#     X_train[i]=scalar.fit_transform(X_train[i].values.reshape(-1, 1))\n#     X_test[i]=scalar.transform(X_test[i].values.reshape(-1, 1))\n\n# X_train.fillna(X_train.mean(),inplace=True)\n# X_test.fillna(X_test.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SEED = 2019\n# np.random.seed(SEED)\n# random.seed(SEED)\n# model = Sequential()\n# model.add(Dense(2 ** 10,input_shape=[X_train.shape[1]], activation='relu'))\n# model.add(Dropout(0.25))    \n# model.add(BatchNormalization())\n# model.add(Dense(2 ** 9, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.25)) \n# model.add(Dense(2 ** 5, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.25))      \n# model.add(Dense(1,activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer='adam') \n\n# history = model.fit(X_train.values, y.values, epochs=10, batch_size=128, verbose=2, shuffle=True)\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb模型融合了五个种子会比较慢\ncol = [i for i in df_train.columns if i not in ['user','label']]\nX_train = df_train[col]\n#X_train['oof'] = oof\ny = df_train['label']\nX_test = df_test[col] \n#X_test['pre'] = prediction\n\noof = np.zeros(X_train.shape[0])\nprediction = np.zeros(X_test.shape[0])\nseeds = [4780, 7830, 4600, 5780, 6170]#(7388-7397)\nnum_model_seed = 5\nfor i,model_seed in enumerate(range(num_model_seed)):\n    oof_cat = np.zeros(X_train.shape[0])\n    prediction_cat=np.zeros(X_test.shape[0])\n    skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n    print(f'第{i}个种子：{model_seed}')\n    for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n        print(index)\n        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n        model = lgb.LGBMClassifier(objective='binary',\n                           boosting_type='gbdt',\n                           num_leaves=32,\n                           max_depth=8,\n                           learning_rate=0.01,\n                           n_estimators=10000,\n                           subsample=0.8,\n                           feature_fraction=0.6,\n                           reg_alpha=10,\n                           reg_lambda=12,\n                           random_state=2019,\n                           is_unbalance=True,\n                           metric='auc')\n        model.fit(\n            train_x, train_y,\n            eval_set=[(train_x, train_y),(test_x, test_y)],\n            eval_metric='auc',\n            early_stopping_rounds=100,\n            verbose=100)\n        oof_cat[test_index] += model.predict_proba(test_x)[:,1]\n        prediction_cat += model.predict_proba(X_test)[:, 1]/5\n    print('AUC',roc_auc_score(y, oof_cat)) \n    oof += oof_cat / num_model_seed\n    prediction += prediction_cat / num_model_seed\nprint('AUC',roc_auc_score(y, oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"su = df_test[['user']].copy()\nsu['prob'] = prediction\nsu.to_csv('submit.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['prob'] = submit['user'].map(dict(zip(df_test['user'], prediction)))\nsubmit.to_csv('submit.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import combinations\ncombins = [c for c in  combinations([2008,2018,48,2000,450,70,90,150,420,30], 5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.metrics import roc_auc_score\ntrain_df = pd.read_csv('../input/train.csv', nrows=40000000)\ntest_df = pd.read_csv('../input/test.csv')\nprint(\"load done\")\n#dask_df = train_df\n#df_pos = dask_df[(dask_df['is_attributed'] == 1)]\n#df_neg = dask_df[(dask_df['is_attributed'] == 0)]\n#df_pos = df_pos.sample(n=5000)\n#print(len(df_pos))\n#print(len(df_neg))\n#df_neg = df_neg.sample(n=2000000)\n#train_df = pd.concat([df_pos,df_neg]).sample(frac=1)\n#del df_pos, df_neg\n#gc.collect()\ndef remove_unkonwn_tag(col, train_df = train_df, test_df = test_df):\n    test_df.loc[~test_df[col].isin(train_df[col]),col] = 9999999\n\ndef remove_lowfreq_tag(col, train_df = train_df,test_df = test_df, N=3):\n    topN_address_list = train_df[col].value_counts()\n    #print(topN_address_list)\n    topN_address_list = topN_address_list[topN_address_list <= N]\n    topN_address_list = topN_address_list.index\n    remove_list = train_df.loc[train_df[col].isin(topN_address_list), col]\n    print('remove:',len(remove_list))\n    print('reserve',len(train_df) - len(remove_list))\n    remove_list = 9999998\n    test_df.loc[test_df[col].isin(topN_address_list), col] = 9999998\n\ndef preprocess(df):\n    df['time_t'] = df.click_time.str[11:13] + df.click_time.str[14:16]\n\nfor i,j in zip(['app','ip','device','os','channel'],[5,4,4,5,5]):\n    remove_lowfreq_tag(i,N=j)\n\n\nfor i in ['app','ip','device','os','channel']:\n    remove_unkonwn_tag(i)\n\nfrom sklearn.preprocessing import LabelEncoder\ndef process_lable_encoder(col, train_df = train_df, test_df = test_df):\n    le = LabelEncoder()\n    le.fit(np.hstack([train_df[col], test_df[col]]))\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n\ncoding_list = ['ip','app','device','os','channel','time_t']\n\npreprocess(train_df)\npreprocess(test_df)\nprint(\"load done\")\n\nfor i in coding_list:\n    process_lable_encoder(i)\n    \nprint(\"label gen done\")\nMAX_IP = np.max(train_df.ip.max()) + 2 #39612 #277396\nMAX_DEVICE = np.max(train_df.device.max()) + 2 #299 #3475\nMAX_OS = np.max(train_df.os.max()) + 2 #161 #3475\nMAX_APP = np.max(train_df.app.max()) + 2 #214 #3475\nMAX_CHANNEL = np.max(train_df.channel.max()) + 2  #155\nMAX_TIME = np.max(train_df.time_t.max()) + 2 #24*60+1\nprint(\"MAX gen done\")\n\ndef get_keras_data(df):\n    X = {\n        'ip': np.array(df.ip),\n\t    'app': np.array(df.app),\n        'device': np.array(df.device),\n        'os': np.array(df.os),\n        'channel': np.array(df.channel),\n        'clicktime': np.array(df.time_t),\n    }\n    return X\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\n\ndef get_model(lr=0.001, decay=0.0):\n    ip = Input(shape=[1], name=\"ip\")\n    app = Input(shape=[1], name=\"app\")\n    device = Input(shape=[1], name=\"device\")\n    os = Input(shape=[1], name=\"os\")\n    channel = Input(shape=[1], name=\"channel\")\n    clicktime = Input(shape=[1], name=\"clicktime\")\n\n    emb_ip = Embedding(MAX_IP, 64)(ip)\n    emb_device = Embedding(MAX_DEVICE, 16)(device)\n    emb_os= Embedding(MAX_OS, 16)(os)\n    emb_app = Embedding(MAX_APP, 16)(app)\n    emb_channel = Embedding(MAX_CHANNEL, 8)(channel)\n    emb_time = Embedding(MAX_TIME, 32)(clicktime)\n\n    main = concatenate([Flatten()(emb_ip), \n                        Flatten()(emb_device), \n                        Flatten()(emb_os),\n                        Flatten()(emb_app),\n                        Flatten()(emb_channel), \n                        Flatten()(emb_time)])\n    main = Dense(128,kernel_initializer='normal', activation=\"tanh\")(main)\n    main = Dropout(0.2)(main)\n    main = Dense(64,kernel_initializer='normal', activation=\"tanh\")(main)\n    main = Dropout(0.2)(main)    \n    main = Dense(32,kernel_initializer='normal', activation=\"relu\")(main)\n    output = Dense(1,activation=\"sigmoid\") (main)\n    #model\n    model = Model([ip, app, device, os, channel, clicktime], output)\n    optimizer = Adam(lr=lr, decay=decay)\n    model.compile(loss=\"binary_crossentropy\", \n                  optimizer=optimizer)\n    return model\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nY_train = train_df.is_attributed.values.reshape(-1, 1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_df[coding_list], Y_train, test_size = 0.1, random_state= 1984, stratify = Y_train)\nX_train = get_keras_data(X_train[coding_list])\nX_valid = get_keras_data(X_valid[coding_list])\n\nprint(\"Defining  model...\")\n\n# Model hyper parameters.\nBATCH_SIZE = 1024*2\nepochs = 1\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(train_df['ip']) / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.001, 0.0003\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\nmodel = get_model(lr=lr_init, decay=lr_decay)\n\nprint(\"Fitting  model to training examples...\")\ncw = {0: 1, 1: 3}\nfor i in range(1):\n    model.fit(\n            X_train, y_train, epochs=1, batch_size=BATCH_SIZE,\n            validation_data=(X_valid, y_valid), verbose=1,class_weight=cw\n    )\n    y_val_pred = model.predict(X_valid)[:, 0]\n    print('Valid AUC: {:.4f}'.format(roc_auc_score(y_valid, y_val_pred)))\n\nX_test = get_keras_data(test_df[coding_list])\npreds = model.predict(X_test, batch_size=BATCH_SIZE)\nsub = pd.DataFrame()\nsub['click_id'] = test_df['click_id']\nsub['is_attributed'] = preds\nsub.to_csv('sub_nn.csv', index=False)\nprint(sub.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"2008 0.7380433590324023\n2018 0.737785476264335\n1 0.7374947713242492\n48 0.7378598000922227\n2000 0.7385279977371564\n704","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"420 AUC 0.7377168137895376\n440 AUC 0.7376184863904467\n450 AUC 0.7383320974716646\n30 AUC 0.7377294852229016\n40 AUC 0.7376122025478872\n60 AUC 0.7373679673898517\n70 AUC 0.7385521015197325\n80 AUC 0.7373743986020773\n90 AUC 0.7378648531033316\n150 AUC 0.7381158036023088","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['prob'] = submit['user'].map(dict(zip(df_test['user'], prediction))) \nsubmit.to_csv('submit1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1、AUC 0.7223035548204311, 0.684129\n2、AUC 0.738926978118269  window27+23+15+gen_user_group_amount_features 70118\n3、AUC 0.7392065631328348 window27+23+15+gen_user_group_amount_features+word2vec_daydiffs  70005\n4、AUC 0.7395118234800954 window27+23+15+gen_user_group_amount_features+gen_user_countvec/tfidf_features(df=df_op, value='op_device') 70094\n5、AUC 0.7400733349184808 gen_user_null_features+2 70054\n6、AUC 0.7396464391857498 word2vec_op_device  70141\n7、AUC 0.7396801408599353 word2vec_op_mo 降\n8、AUC 0.7403186712226607 word2vec_op_type 降\n9、AUC 0.7398117360769147 word2vec_op_channel 降\n10、AUC 0.7402225579054323 word2vec_trans_platform 降\n11、AUC 0.7409563055778169 word2vec_tunnel_in 降 70115\n12、AUC 0.7389892812972378 window=9和2 702525\n13、AUC 0.7396 gen_user_group_amount_features(df=df_trans, value='tunnel_in/tunnel_out' 70245\n14、AUC 0.7399620955787308 gen_user_tfidf/countvec_features(df=df_trans, value='amount') 降\n15、AUC 0.741150201936527 Word2vec32 降\n16、AUC 0.740550201936527 gen_user_group_amount_features(df=df_trans, value='tunnel_in/tunnel_out')+Word2vec32 703591\n17、融合1516 703702","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC 0.7406062286129351 \n2次伪标签提示4个万分点","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}